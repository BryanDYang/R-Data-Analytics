---
title: "Module 7 Group Assignment"
author: "Shawn Abdian, Lexie Brown, Jenna Swiecki, Carolina Trevino, Saeeda Zaman,Bryan Yang"
date: "`r Sys.Date()`"
output: html_document
---
# Overview

This assignment is based upon the data, R code and RStudio notebook, and discussion in the live session for this module. We recommend that you attend or watch the live session for this module before attempting this assignment.      

In addition to being concerned about profit (as discussed in class), NANSE is concerned about predicting higher traffic in its stores. That is, NANSE wants to predict which weeks and stores will sell above the median number of units.      

Use the code discussed in class to examine the effect of the same variables used in module 6 to predict the target variable `high_med_units` (as discussed in the data description sheet listed above, this variable is an indicator variable that equals 1 when the number of units sold for that store for that week was above the median and 0 otherwise).      

Suggestion: While you could just cut and paste the code used in the high engagement session for this module, we encourage you to write all of the code from scratch. This is a technique we often use when looking up and using new code we are borrowing from someone else.)    

## Confusion Matrix Function & Loading Data

```{r}
# Confusion Matrix Function
my_confusion_matrix <- function(cf_table) {
  true_positive <- cf_table[4]
  true_negative <- cf_table[1]
  false_positive <- cf_table[2]
  false_negative <- cf_table[3]
  accuracy <- (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
  sensitivity_recall <- true_positive / (true_positive + false_negative)
  specificity_selectivity <- true_negative / (true_negative + false_positive)
  precision <- true_positive / (true_positive + false_positive)
  neg_pred_value <- true_negative / (true_negative + false_negative)
  print(cf_table)
  my_list <- list(sprintf("%1.0f = True Positive (TP), Hit", true_positive),
                  sprintf("%1.0f = True Negative (TN), Rejection", true_negative),
                  sprintf("%1.0f = False Positive (FP), Type 1 Error", false_positive),
                  sprintf("%1.0f = False Negative (FN), Type 2 Error", false_negative),
                  sprintf("%1.4f = Accuracy (TP+TN/(TP+TN+FP+FN)", accuracy),
                  sprintf("%1.4f = Sensitivity, Recall, Hit Rate, True Positive Rate (How many positives did the model get right? TP/(TP+FN))", sensitivity_recall),
                  sprintf("%1.4f = Specificity, Selectivity, True Negative Rate (How many negatives did the model get right? TN/(TN+FP))", specificity_selectivity),
                  sprintf("%1.4f = Precision, Positive Predictive Value (How good are the model's positive predictions? TP/(TP+FP))", precision),
                  sprintf("%1.4f = Negative Predictive Value (How good are the model's negative predictions? TN/(TN+FN)", neg_pred_value)
  )
  return(my_list)
}

#Load Packages 
library(tidyverse)
#Load Data
df <- read_rds('mod6HE_logit.rds')

```

## KNN Algorithm   

### Set-Up
```{r}
# For Back-Test
knn1 <- df %>% ungroup() %>% 
  select(store, week, region, high_med_rev, high_med_units, high_med_gpm)

# make the tarkget feature a factor and put the "low" level first so the confusion matrix works correctly
knn2 <- df %>% mutate(high_med_units = factor(if_else(high_med_units==1, 'high', 'low'), levels=c('low', 'high')))
knn2 <- knn2 %>% ungroup() %>%
  select(high_med_units, size, region, promo_units_per, 
         altbev_units_per, confect_units_per, salty_units_per,
         velocityA_units_per, velocityB_units_per, velocityC_units_per,
         velocityD_units_per, velocityNEW_units_per)

# Data must be numeric so one-hot encode 'region'
library(fastDummies)
knn2 <- fastDummies::dummy_cols(knn2, select_columns = c("region"), remove_selected_columns = T)

# Check that "positive" is last for the confusion matrix to work
contrasts(knn2$high_med_units)

```
### Partition the Data & Z-Score Standardization

```{r}
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=knn2$high_med_units, p=0.75, list=F)
data_train <- knn2[partition,]
data_test <- knn2[-partition,]

# Separate the target variable from the training and testing data
x_train <- data_train %>% select(-high_med_units)
x_test <- data_test %>% select(-high_med_units)
y_train <- data_train$high_med_units
y_test <- data_test$high_med_units

# Features must be standardized so use z-score standardization
x_train <- scale(x_train)
x_test <- scale(x_test)

```

### Run the KNN Model, Confusion Matrix & Check the accuracy
```{r}
library(class)
knn_prediction = class::knn(train=x_train, test=x_test, cl=y_train,
                            k=round(sqrt(nrow(data_train))/2))

table2 <- table(knn_prediction, y_test)
my_confusion_matrix(table2)
```

### Back-Test: Put the data back together for future use
```{r}
# Put the prediction back into the test data
data_test$knn <- knn_prediction

# Create a variable that shows if the prediction was correct
data_test <- data_test %>%
  mutate(correct_knn = if_else(knn == high_med_units, 'correct', 'WRONG!'))

# Add back the original data to the test data
temp1 <- knn1[-partition,]
full_test_knn <- bind_cols(temp1, data_test)

# Test View
full_test_knn <- full_test_knn %>%
  select(store, week, high_med_units...5, knn, correct_knn, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_knn, n=20)
```


1. (0.5 points) Is type 1 or type 2 error higher for the KNN model? That is, which has a higher number?    

**Type 1 Error (False Positive) is higher than Type 2 Error (False Negative). There are 324 Type 1 Error versus 215 Type 2 Error.**

2. Which aspect of the accuracy of the KNN model is better—sensitivity (hit rate) or specificity (true negative rate)?    

2.a.(0.5 Points) Write your answer here.   

**Sensitivity (True Positive Rate) has better accuracy than Specificity (True Negative Rate). Sensitivity is 82.90% and Specificity is 74.29%.**

2.b.(0.5 Points) Explain the above answer. That is, write one or two sentences that explain which individual components of these measures (either true positive, true negative, false positive, or false negative) led to sensitivity (hit rate) or specificity (true negative rate) being better than the other.    

**Sensitivity = TP/(TP+FN). There are 1042 TP & 215 FN. TP is almost five times greater than FN. Big difference in TP relative to FN led to higher Sensitivity of 82.90%.**     

**Specificity = TN/(TN+FP). There are 936 TN & 324 FP. TN is almost three times greater than FP. Lower difference of TN to FP, relative to Sensitivity components, led to lower Specificity of 74.29%.**    

2.c.(0.5 Points) What does this mean about the business that this model is examining? That is, in less than three sentences, discuss the business implications for NANSE for your answer in 2b.   

**This means NANSE can trust the model's prediction of high_med_units 1 (True) more than 0 (False). NANSE has more certainty of predicting stores that will sell above median units than stores that won't sell above median units.**

2.d.(1 Point) The nine measures of accuracy provided in the ` my_confusion_matrix()` function output are not the only measures of accuracy. Do an internet search and describe two other measures of accuracy. These measures may or may not be derived from these nine measures, but must not be one of these nine measures.   

**According to the source cited below, there are other variables other than what `my_confusion_matrix()` shows. Two of them are Positive Likelihood Ratio (LR+ = TPR(Sensitivity, True Positive Rate) / FPR(False Positive Rate)) & Negative Likelihood Ratio (LR- = FNR(False Positive Rate) / TNR(Specificity, True Negative Rate).**   

**Source: https://en.wikipedia.org/wiki/Confusion_matrix**    

## Decision Tree Algorithm  

### Set-Up
```{r}
# For Back-Test 
tree1 <- df %>% ungroup() %>%
  select(store, week, high_med_rev, high_med_units, high_med_gpm, high_med_gp)

# make the target feature and 'region' a factor
tree2 <- df %>% mutate(high_med_units = factor(if_else(high_med_units == 1, 'high', 'low'), levels = c('low', 'high')),
                       region = factor(region))
tree2 <- tree2 %>% ungroup() %>%
  select(high_med_units, size, region, promo_units_per,
         altbev_units_per, confect_units_per, salty_units_per,
         velocityA_units_per, velocityB_units_per, velocityC_units_per,
         velocityD_units_per, velocityNEW_units_per)

# Check positive is last for the confusion matrix to work
contrasts(tree2$high_med_units)

```

### Partion Data   

```{r}
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=tree2$high_med_units, p=0.75, list=F)
data_train <- tree2[partition,]
data_test <- tree2[-partition,]

```

### Train the Model
```{r}
library(rpart)
library(rpart.plot)
model_tree <- rpart::rpart(high_med_units ~., data_train)
```

### Use trained model to predict whether 'high_med_units' is high or low
```{r}
predict_tree <- predict(model_tree, data_test, type='class')
```

### Use the confusion matrix to examine the accuracy of the model
```{r}
table1 <- table(predict_tree, data_test$high_med_units)
my_confusion_matrix(table1)
```

### Plot the Decision Tree
```{r}
rpart.plot::rpart.plot(model_tree, box.palette = 'RdBu', shadow.col = 'grey', nn=T)
```
### Back-Test: Put the data back together for future use
```{r}
# Put the prediction back into the test data
data_test$tree <- predict_tree

# Create a variable that shows if the prediction was correct
data_test <- data_test %>%
  mutate(correct_tree = if_else(tree == high_med_units, 'correct', 'WRONG!'))

# Add back the original data
temp1 <- tree1[-partition,]
full_test_tree <- bind_cols(temp1, data_test)

# For Viewing
full_test_tree <- full_test_tree %>%
  select(store, week, high_med_units...4, tree, correct_tree, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_tree, n=20)

```


3. Interpret the decision tree output by answering the following questions:    

3.a.(1 Points) Start at the beginning of the tree, the root node. What is the most important factor for above median units sold? That is, if you had to pick one feature that told you the most, what would it be? Said even another way, which feature keeps popping up in the tree?    

**From start to the root node, 'size' keeps popping up in the tree. If I had to pick one feature that told me the most, it has to be the 'size' offering of products. This means the average per week number of unique products available for sale from that store is important.**   

3.b.(1 Points) Using the decision tree, if you are a smaller store (a store that offers less than 980 products for sale), which regions are more likely to lead to above median units sold?   

**According to the decision tree, Quebec and Vancouver will more likely to lead to above median units sold.**

4. Look back to your assignment for Module 6 in which you were asked to use logistic regression to examine median units sold (`high_med_units`). Over the course of that assignment and this current assignment you have used three algorithms (logistic regression, KNN and decision trees) to examine this feature variable (`high_med_units`). Suppose you were working on an engagement to help NANSE better predict and understand when a store in a particular week will sell an above average number of units. In particular, suppose NANSE engaged you to answer the questions below. Using the results from your last assignment with logistic regression and the results from the current assignment, respond to each question by picking which of the three algorithms (logistic regression, KNN or decision trees) is the most helpful in answering the question and explain why by comparing and contrasting to the other models. Even though your client is unlikely to care about this, for pedagogical purposes, make sure to discuss what it is about the model that you pick that makes it more helpful than the others. Your response to a. and b. should be about three sentences each.       

4.a.(1 Point) “We would like to build a company-wide dashboard next year that tells us at the end of each week which stores sold enough units to be in the top half of units sold for that year, even though the year is not over. Can you use the data from the year that just ended to create a predictive model that, with a high degree of accuracy, tells us which of our stores in a given week is likely to sell above median units?”        

**To predict which stores in a given week is likely to sell above median units based on the model created by previous year data with high degree of accuracy, logistic regression is most suited. With the same data, logistic regression has 95.19% accuracy and all the other 9 measures compared to KNN (78.59%) and decision tree (75.17%). However, decision tree may help to visualize and identify actionable items with its nodes.**

4.b.“In addition to this dashboard, we would like to use last year’s data to understand which variables help our stores have successful weeks. Can you use that data to tell us which factors are most important at helping our stores have above median units sold in a given week?”       

**Logistic regression is superior than KNN and decision tree algorithms to create model to specify which variables help the stores have successful weeks and by how much with its coefficients. KNN was quick and we were able to 'test' and 'train' at the same time. KNN model, also, considers z-value in our case. However, there are still not enough information to see how specific variables helped the stores by how much. Also, although our decision tree model helps NANSE visualize the actionable items such as which scenarios would be the most optimal at specific scenario, it's not detailed enough to quantify the measure of success.**

5. Our discussions on Coursera and in class have focused on three classification algorithms (logistic regression, k-nearest neighbors, and decision trees). Many other classification algorithms exist and new ones are being developed all of the time. As you continue to expand your skills you will need to develop the ability to use the framework we are providing in this class to learn new algorithms. Do some research and find a classification algorithm that we have not discussed (not one of these three).    

5.a.(0.5 Points) List this algorithm.   

**Stochastic Global Optimization**   

5.b.(2 Points) List several advantages and disadvantages of this algorithm. If possible, compare and contrast it to the three algorithms we studied.   

**StoGo is a an algorithm that works by systematically dividing the search space into smaller hyper-rectangles via a branch-and-bound technique, and searching them by a gradient-based local-search algorithm including some randomness. Some of the advantages of the algorithms are easier fitting in the memory due to a single training example being processed by the model. It is computationally fast as only one sample is processed at a time. For larger dataset, it can converge faster as it updates the parameters more frequently. Some the disadvantages are lose of advantages of vectorized operations as it deals with only a single example at a time. Frequent updates are computationally expensive because of using all resources for processing one training sample at a time. Based on the description of the algorithm, it seems to have both features of KNN and decision tree with the consideration of randomness (outliers) and optimizing with uncertainty .**      

5.c.(0.5 Points) Find two relevant lines of R code that are used to run this algorithm and paste them below.   

#install.packages("nloptr")    
library(nloptr)    

Usage:   
rf <- stogo(
  x0,    
  fn,    
  gr = NULL,    
  lower = NULL,    
  upper = NULL,    
  maxeval = 10000,    
  xtol_rel = 1e-06,    
  randomized = FALSE,    
  nl.info = FALSE,    
  ...    
)    
 
**Sources: https://cswr.nrhstat.org/stochopt & https://rdrr.io/cran/nloptr/man/stogo.html**
