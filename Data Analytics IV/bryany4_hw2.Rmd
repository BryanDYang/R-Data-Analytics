---
title: "Module 2 Individual Assignment"
author: "Bryan Yang"
date: "`r Sys.Date()`"
output: html_document
---

1a. Using the dataset we cleaned up and used in class (called "...EPS rounding_after class" and provided for you as well as the data description sheet, "...Data Description Sheet_EPS rounding_after class.xlsx"), graph a bar chart that shows separate plots for the frequency of the EPS digit after the decimal point (called "digit_diluted" in the dataset) based upon the quarters of the year (1-4). Thus, you will have four separate plots or one plot with 4 separate panels or facets. Hint: We did this in class but had a separate plot for each year instead of for each quarter. For R, use `facet_wrap()`.


```{r}
# Step 1: Bring in and explore data
library(tidyverse)
df1 <- read_rds("assign_data_EPS rounding_after class.rds")
head(df1, 10)
str(df1)

# Step 2: Calculate diluted EPS in cents
# 100 * (income after extraordinary items ('IBADJQ' + 'XIDOQ')) / number of common shares used to calculate diluted EPS ('CSJFDQ')
df1 <- df1 %>%
  mutate(EPS_diluted_cents = ((ibadjq + xidoq) * 100) / cshfdq)

# Step 3: Extract the digit
# 'stringr::str_sub()' is a tidyverse function that extracts substrings from text 
#       start and end points are inclusive
# 'stringr::str_locate()' is a tidyverse function that locates a substring in text
# '\\.' is regular expression code to find the decimal point

df1 <- df1 %>%
  mutate(digit_diluted = stringr::str_sub(as.character(EPS_diluted_cents),
                                          start = stringr::str_locate(as.character(EPS_diluted_cents), '\\.') +1,
                                          end = stringr::str_locate(as.character(EPS_diluted_cents), '\\.') +1))

# Step 4: Keep EPS > 0.1
df1 <- df1 %>%
  filter(EPS_diluted_cents > 0.1)

# Step 5: Group by year, quarter, and digit and then count occurrences
df2 <- df1 %>%
  group_by(fyearq, fqtr, digit_diluted) %>%
  summarize(count_filings = n())

# Step 6: Plot the counts by digit
df2 %>%
  ggplot(aes(x = digit_diluted, y = count_filings)) +
  geom_col(aes(fill = digit_diluted)) + theme_bw()

# Step 7: Separate plots by quarters
df2 %>%
  ggplot(aes(x = digit_diluted, y = count_filings)) +
  geom_col(aes(fill = digit_diluted)) +
  facet_wrap(vars(as_factor(fqtr))) + theme_bw()
  
```

1b. Which quarter or quarters have the smallest difference in the frequency of integer 4 relative to integer 5? You don't need to calculate this difference, just look at the four plots and make your best judgment.

**Quarter 4 seems to have the smallest difference in the frequency of integer 4 relative to integer 5.** 

2a. Next, create and examine these same sets of plots for just 1998. Thus, you will have four sets of graphs for that year.

```{r}
df3 <- df2 %>%
  filter(fyearq == 1998)
df3 %>%
  ggplot(aes(x = digit_diluted, y = count_filings)) +
  geom_col(aes(fill = digit_diluted)) +
  facet_wrap(vars(as_factor(fqtr)))
```
     
**The differences in the frequency of the integer 4 and 5 are more noticeable in each quarter for 1998 than the aggregated data graph from 1998 to 2020.**

2b. Next, create and examine this same set of plots for just 2019. Thus, you will have four sets of graphs for that year.

```{r}
df4 <- df2 %>%
  filter(fyearq == 2019)
df4 %>%
  ggplot(aes(x = digit_diluted, y = count_filings)) +
  geom_col(aes(fill = digit_diluted)) +
  facet_wrap(vars(as_factor(fqtr)))
```
      
**The differences in the frequency of the integer 4 and 5 are very similar for quarter 1 and 2 for 2019 to the aggregated data graph from 1998 to 2020. However, there are very small difference in the frequency of the integer 4 and 5 for 2019.**

2c. What differences do you notice between 1998 and 2019 in the frequency of integer 4 and the frequency of integer 4 relative to integer 5?

**Observing the integer 4 for both 1998 and 2019, integer 4 had increased whereas integer 5 seemed to stayed the same or decreased. Also, in 1998, the difference of integer 4 and 5 are much more drastic compared to 2019.**

3. Next, use your analytics skills to find and report the worst 10 offenders of EPS rounding to the SEC.

* Specifically, create a table of the 10 worst offenders following the criteria _below.

* You will need to create a table that has one observation per company. Thus, you will need to aggregate each company's results down to one row using a summarize function.

* Additionally, you will only include a company in the table if the aggregated results from that company meet the following criteria:

  + The company has more than 56 observations (quarters/rows) in the original, "EPS rounding_after class," dataset.
  
  + Less than 1.18% of all of the company’s observations have a 4 in the digit after the decimal point of EPS (e.g., in the `EPS_diluted_cents` column).
  
  + More than 11% of all of the company’s observations have a 5 in the digit after the decimal point of EPS (e.g., in the `EPS_diluted_cents` column).
  
```{r}
# Create columns for 4 & 5 in the digit after decimal point of EPS in TRUE or FALSE
df5 <- df1 %>%
  mutate(
    EPS_digit_4 = digit_diluted == 4,
    EPS_digit_5 = digit_diluted == 5
  )

# Convert columns for 4 & 5 in the digit after decimal point of EPS to 1 or 0
df5$EPS_digit_4 <- ifelse(df5$EPS_digit_4 == TRUE, 1, 0)
df5$EPS_digit_5 <- ifelse(df5$EPS_digit_5 == TRUE, 1, 0)
```

  
```{r}
# Aggregate the dataframe in company per row
df5 <- df5 %>%
  select(tic, conm, EPS_digit_4, EPS_digit_5) %>%
  group_by(tic, conm) %>%
  summarize(
            EPS_digit_4 = sum(EPS_digit_4),
            EPS_digit_5 = sum(EPS_digit_5),
            total_obs = n()
            ) %>%
  ungroup()

# Create columns for 4 & 5 in the digit percentage to the total observations per company
df5 <- df5 %>%
  mutate(
    EPS_digit_4_per = round(EPS_digit_4 / total_obs, digits = 4),
    EPS_digit_5_per = round(EPS_digit_5 / total_obs, digits = 4)
  )
```
```{r}

```



```{r}
# Filter based on the criteria above
df5 <- df5 %>%
  filter(
    # More than 56 observations (quarters/rows)
    total_obs > 56,
    # Less than 1.18% of all of the company's observations have a 4 in the digit after the decimal point of EPS
    EPS_digit_4_per < .0118,
    # More than 11% of all of the company's observations have a 5 in the digit after the decimal point of EPS
    EPS_digit_5_per > .11
    )
```

  
3a. Submit this table. At the minimum, the table (or a screenshot of the table) must contain the ticker (tic), company name (conm), percentages of observations with 4 in the first digit after the decimal, and percentages of observations with 5 in the first digit after the decimal._

```{r}
# Show table of the 10 worst offenders following the criteria above
df5[1:10,]
```

3b. What observations can you make about these companies?_

**Out of 384,308 companies, there are only nine companies that met all three criteria. Despite of their high quarterly filing status (more 56), we can observe these companies have 0 or 1 filing with 4 in the digit after the decimal point of EPS_diluted_cents (less than 1.18%) compared to their 5 in the digit after the decimal point of EPS_diluted_cents (more than 11%). This is suspicious and we need to dive further into their financial statements and supplemented notes.**

3c. Are the three companies charged by the SEC (FULT, HCSG, and TILE) in the following article in your table? A simple "yes" or "no" is fine. You do not have to answer this question. We will not grade your 3c._

**Yes, out of three (FULT, HCSG, TILE), we can see FULT & TILE.**

